# Controllable-Text-to-Image-System-
### High-Performance Controllable Text-to-Image Generation System (SD/SDXL, LoRA, ControlNet, ONNX/TensorRT Optimization, FastAPI Serving)

This repository contains a **production-grade, controllable text-to-image system** built on  
**Stable Diffusion**, with **LoRA fine-tuning**, **ControlNet conditioning**,  
**high-performance inference (ONNX Runtime/TensorRT)**,  
and a **FastAPI serving layer** with **Prometheus monitoring**.

> âš¡ Designed to bring text-to-image **from research â†’ production**  
> with **fast**, **stable**, **controllable**, and **deployable** generation.

---

# ğŸ“Œ Table of Contents
- [1. Overview](#-overview)
- [2. Key Features](#-key-features)
- [3. System Architecture](#-system-architecture)
- [4. Quick Start](#-quick-start)
- [5. Demo (UI + API)](#-demo-ui--api)
- [6. Performance](#-performance)
- [7. Training (LoRA)](#-training-lora)
- [8. Controllability (ControlNet)](#-controllability-controlnet)
- [9. Safety & Content Filtering](#-safety--content-filtering)
- [10. Deployment (Docker)](#-deployment-docker)
- [11. Monitoring (Prometheus)](#-monitoring-prometheus)
- [12. Project Roadmap](#-project-roadmap)
- [13. File Structure](#-file-structure)
- [14. License](#-license)

---

# ğŸ” Overview

This project builds a **complete AI generation system**, including:

- **Controllable image generation** (ControlNet)
- **Efficient fine-tuning** (LoRA)
- **High-performance inference** (torch.compile + xFormers + ONNX + TensorRT)
- **Production service** (FastAPI)
- **Monitoring & reliability** (Prometheus metrics, concurrency control)
- **User interface** (Gradio)
- **Safety filtering pipeline**

The system supports:

- **CFG / Steps / Seed / Resolution**
- **LoRA weight switching**
- **ControlNet conditioning (Canny / Depth / OpenPose)**
- **Batch generation**
- **ONNX/TensorRT Acceleration**

---

# ğŸš€ Key Features

### ğŸ§© **1. Multi-level Controllability**
- ControlNet: Canny / Depth / OpenPose  
- Adjustable control strength  
- CFG scale, inference steps, resolution  

### ğŸ¨ **2. LoRA Fine-tuning Support**
- Plug-and-play LoRA  
- Rank / Î± ablation  
- Custom style & domain adaptation  

### âš¡ **3. High-Performance Inference**
- `torch.compile()` acceleration  
- xFormers flash attention  
- ONNX Runtime GPU  
- TensorRT FP16/INT8 (optional)  
- Batch & concurrency optimization  

### ğŸ—ï¸ **4. Production Serving (FastAPI)**
- `/generate` endpoint  
- `/health`  
- `/metrics` (Prometheus format)  
- Internal queue + rate limiting  
- Retry logic  

### ğŸ›¡ï¸ **5. Safety Filtering**
- Keyword/regex filter  
- Optional multi-label classifier  
- Structured refusal messages  

### ğŸ“Š **6. Monitoring (Prometheus)**
- Latency (P50/P95/P99)  
- Throughput (TPS/RPS)  
- GPU memory  
- Failure rate  

---

# ğŸ›ï¸ System Architecture


The system consists of:

- **Training Layer** â†’ LoRA/Data Prep  
- **Control Layer** â†’ ControlNet modules  
- **Inference Layer** â†’ PyTorch / ONNX / TensorRT pipelines  
- **Serving Layer** â†’ FastAPI service + queue  
- **UI Layer** â†’ Gradio interface  
- **Evaluation Layer** â†’ CLIPScore/FID + Latency/Throughput tests  
- **Safety Layer** â†’ content filtering + rejection policies  

## Architecture Diagram
<img width="1348" height="1876" alt="architecture_v1" src="https://github.com/user-attachments/assets/6dc14d51-62c5-491d-8be4-a45d43cedb0c" />

# Environment Setup
- GPU: Tesla T4 (via Google Colab)
- CUDA: 12.4
- Frameworks:
    torch==2.x
    diffusers==0.x
    transformers==4.x
    accelerate==0.x
![GPU](docs/environment_gpu_t4.png)
![Inference](docs/sd15_inference_cat_moon_v1.png)
---

# âš¡ Quick Start

## 1. Install Environment
```bash
pip install torch diffusers transformers accelerate xformers
pip install fastapi uvicorn pillow opencv-python
pip install onnxruntime-gpu
pip install gradio
```

## 2. Run Basic SD Inference
python inference/pipeline_pt.py

## 3. Launch Gradio UI
python ui/gradio_app.py

## 4. Start API Server
uvicorn serving.app:app --host 0.0.0.0 --port 8000

ğŸ–¥ï¸ Demo (UI + API)
âœ¨ Gradio UI

ğŸ”Œ FastAPI Endpoints
POST /generate
{
  "prompt": "a futuristic city in sunset",
  "steps": 20,
  "cfg_scale": 7.5,
  "controlnet": "canny",
  "strength": 0.8
}

GET /health
{"status": "ok"}

GET /metrics

Prometheus-style metrics.

ğŸ“ˆ Performance
ğŸ”¥ PyTorch Optimized vs ONNX Runtime

Model	Device	Steps	P95 Latency	TPS
PyTorch FP16	
torch.compile + xformers	
ONNX Runtime (FP16)
ğŸ§ª Training (LoRA)
Train Your Own LoRA
python training/lora_train.py --config training/config.yaml

Example Results

ğŸ›ï¸ Controllability (ControlNet)
Input â†’ Output Example

ğŸ›¡ï¸ Safety & Content Filtering
Pipeline

Rules

Keyword filter

Regex rules

Optional multi-label classifier

Structured refusal messages (policy_id, reason)

ğŸ³ Deployment (Docker)
Build
docker build -t t2i .

Run
docker run -p 8000:8000 t2i


ğŸ“¡ Monitoring (Prometheus)

Prometheus exposes:

latency_ms

throughput_rps

gpu_memory_mb

failure_rate


ğŸ—ºï¸ Project Roadmap

 SDXL support

 gRPC serving

 TensorRT INT8 pipelines

 LoRA adapter merging

 Real-time A/B human evaluation interface

 HuggingFace Spaces deployment

ğŸ“ File Structure
```bash
t2i-controllable-fast
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ lora_train.py
â”‚   â”œâ”€â”€ data_prep.py
â”‚   â””â”€â”€ config.yaml
â”œâ”€â”€ control/
â”‚   â”œâ”€â”€ apply_controlnet.py
â”‚   â””â”€â”€ adapters/
â”œâ”€â”€ inference/
â”‚   â”œâ”€â”€ pipeline_pt.py
â”‚   â”œâ”€â”€ export_onnx.py
â”‚   â””â”€â”€ pipeline_onnx.py
â”œâ”€â”€ serving/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ queue.py
â”‚   â””â”€â”€ monitor.py
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ gradio_app.py
â”œâ”€â”€ eval/
â”‚   â”œâ”€â”€ quality.py
â”‚   â”œâ”€â”€ latency_throughput.py
â”‚   â””â”€â”€ human_eval.md
â”œâ”€â”€ safety/
â”‚   â”œâ”€â”€ content_filter.py
â”‚   â””â”€â”€ policy.yaml
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture_v1.png
â”‚   â”œâ”€â”€ environment_gpu_t4.png
â”‚   â”œâ”€â”€ sd15_inference_cat_moon_v1.png
â”‚   â”œâ”€â”€ gradio_v1.png
â”‚   â”œâ”€â”€ lora_comparison.png
â”‚   â”œâ”€â”€ controlnet_canny_comparison.png
â”‚   â””â”€â”€ perf_onnx_vs_pt.png
â””â”€â”€ README.md
```

ğŸ”‘ License

MIT License.
Feel free to use, modify, or deploy.
